import streamlit as st
from stateful_chat.session import ChatSession,InstructFormat
import ollama
import uuid
import json
import os

# app config
st.set_page_config(page_title="Stateful Chatbot", page_icon="ðŸ¤–")
st.title("Stateful Chatbot")

# initialize session state
if "chat_session" not in st.session_state:
    cs = ChatSession(str(uuid.uuid4()))
    cs.llm = "llama3.1:8b-instruct-q4_K_S"
    cs.stop_words = ["<|begin_of_text|>","<|start_header_id|>","<|eot_id|>"]
    cs.system_prompt = ""
    st.session_state.chat_session = cs

# make sure regenerate flag is initialized to 'False'
if "needs_regen" not in st.session_state:
    st.session_state.needs_regen = False

# make sure continue flag is initialized to 'False'
if "needs_continue" not in st.session_state:
    st.session_state.needs_continue = False

# callback to set regenerate flag
def request_regen():
    st.session_state.needs_regen = True
    
# callback to set regenerate flag
def request_continue():
    st.session_state.needs_continue = True

def load_state():
    """Load saved session state."""
    session_file = st.session_state.session_loader
  
    if session_file is None:
        st.warning("No settings file selected!")
        return
    st.session_state.chat_session = ChatSession.from_json(session_file)

# method to load all available instruct formats so we can offer them to the user
def load_instruct_formats():
    # get paths to files
    fmt_dir = "./instruct_formats/"
    fmt_files = [(os.path.join(fmt_dir, file)) for file in os.listdir(fmt_dir)]
    # load each file into a list
    fmt_obj = []
    for ff in fmt_files:
        inst_fmt = InstructFormat.from_json(open(ff, mode='r'))
        fmt_obj.append(inst_fmt)
    return fmt_obj

def get_response():
    o_gen = st.session_state.chat_session.get_rag_response(n_msg=2, rag_thresh=0.3, n_results=1, stream=True)
    #o_gen = st.session_state.chat_session.get_response(stream=True)
     # make wrapper
    def ogen_wrapper(o_gen):
        for chunk in o_gen:
            yield chunk['response']
        st.write("Response generation rate: " + str(round(chunk['eval_count']/chunk['eval_duration']*1e9, 2)) + " Tk/sec")
        st.session_state.context_length = chunk['prompt_eval_count'] + chunk['eval_count']
        st.write("Context length: " + str(st.session_state.context_length))

    return ogen_wrapper(o_gen)

def get_continuation():
    o_gen = st.session_state.chat_session.continue_response(stream=True)
     # make wrapper
    def ogen_wrapper(o_gen):
        for chunk in o_gen:
            yield chunk['response']
        st.write("Response generation rate: " + str(round(chunk['eval_count']/chunk['eval_duration']*1e9, 2)) + " Tk/sec")
        st.session_state.context_length = chunk['prompt_eval_count'] + chunk['eval_count']
        st.write("Context length: " + str(st.session_state.context_length))

    return ogen_wrapper(o_gen)

# system prompt
#st.session_state.system_prompt = st.text_area("System prompt:", value = "You are a helpful assistant.")

# stop token list
#st.session_state.chat_session.stop_words = st.text_input("Comma-separated stop words:", ",".join(st.session_state.chat_session.stop_words)).split(",")

# Construct tabs
tab_main, tab_mem, tab_db = st.tabs(["Main", "Memory", "Database"])
# =============== Main Tab ========================

with tab_main:
    # LLM model to use
    st.session_state.chat_session.llm = st.text_input("LLM:", st.session_state.chat_session.llm)

    # chat format to use
    if "instruct_formats" not in st.session_state:
        # load available formats
        st.session_state.instruct_formats = load_instruct_formats()
        st.session_state.instruct_names = (fmt.name for fmt in st.session_state.instruct_formats)
    # format selector
    st.session_state.chat_session.instruct_format = st.selectbox("Instruct format to use:", options=st.session_state.instruct_formats,
                                                                format_func=lambda fmt: fmt.name)
    # Base system prompt
    st.session_state.chat_session.system_prompt = st.text_area("System prompt:", st.session_state.chat_session.system_prompt, height = 25)
    
    # conversation roles
    st.session_state.chat_session.user_role = st.text_input("User role:", st.session_state.chat_session.user_role)
    st.session_state.chat_session.ai_role = st.text_input("AI role:", st.session_state.chat_session.ai_role)
    
    # mode-switching check-box
    if st.checkbox(label = "Manual editing mode"):
        # we're using manual mode
        # convert conversation to text and stick it in a text editor
        formatted_text = st.session_state.chat_session.format_readable()
        edited_text = st.text_area("Edit raw conversation text:", formatted_text, height = 500)
        # when the text area changes, put the new version into the session
        st.session_state.chat_session.import_readable(edited_text)
    else:
        # we're in automatic mode
        # do we need to regenerate a message?
        # if so, we need to drop the last message before loading history
        if st.session_state.needs_regen:
            # delete old response
            del st.session_state.chat_session.messages[-1]
        # now render conversation history in container
        with st.container(height=500):
            # display conversation
            if len(st.session_state.chat_session.messages) > 0:
                for msg in st.session_state.chat_session.messages:
                    with st.chat_message(msg['role']):
                        st.write(msg['content'])
            # if we're regenerating, do that now
            if st.session_state.needs_regen:
                with st.chat_message(st.session_state.chat_session.ai_role):
                    response = st.write_stream(get_response())
                    st.session_state.chat_session.append_message(role=st.session_state.chat_session.ai_role,
                                                                 content=response)
                    #st.session_state.chat_session.messages.append({"role": st.session_state.chat_session.ai_role, "content": response})
                # reset flag
                st.session_state.needs_regen = False
                # embed regenerated AI response
                st.session_state.chat_session.embed_text(st.session_state.chat_session.messages[-1], "message")
            # if we're continuing, do that now
            if st.session_state.needs_continue:
                with st.chat_message(st.session_state.chat_session.ai_role):
                    response = st.write_stream(get_continuation())
                    st.session_state.chat_session.append_message(role=st.session_state.chat_session.ai_role,
                                                                 content=response)
                    #st.session_state.chat_session.messages.append({"role": st.session_state.chat_session.ai_role, "content": response})
                # reset flag
                st.session_state.needs_continue = False
                # embed continued AI response
                st.session_state.chat_session.embed_text(st.session_state.chat_session.messages[-1], "message")
            # finally, provide chat input and regen buttons
            user_query = st.chat_input("Type your message here...")
            if user_query is not None and user_query != "":
                st.session_state.chat_session.append_message(role=st.session_state.chat_session.user_role,
                                                             content=user_query)
                #st.session_state.chat_session.messages.append({"role": st.session_state.chat_session.user_role, "content": user_query})
            
                with st.chat_message(st.session_state.chat_session.user_role):
                    st.markdown(user_query)
            
                with st.chat_message(st.session_state.chat_session.ai_role):
                    response = st.write_stream(get_response())
                    st.session_state.chat_session.append_message(role=st.session_state.chat_session.ai_role,
                                                                 content=response)
                # embed user message
                st.session_state.chat_session.embed_text(st.session_state.chat_session.messages[-2], "message")
                # embed AI response
                st.session_state.chat_session.embed_text(st.session_state.chat_session.messages[-1], "message")
            # regenerate last message button
            st.button(label="Regenerate", on_click=request_regen)
            # regenerate last message button
            st.button(label="Continue", on_click=request_continue)
        # end chat container
    # end auto/manual selection

    # load button
    uploaded_file = st.file_uploader(label="Load a saved session", key = "session_loader", on_change = load_state)
    
    # save button
    button_download = st.download_button(label="Save Settings",
                                         data = st.session_state.chat_session.to_json(),
                                           file_name=f"settings.json",
                                         key = "session_saver",
                                           help="Click to Download Current Settings")
    
    
    # manually generate summary of N oldest messages
    num_summ = st.number_input("Number of messages to summarize and archive:",
                               min_value=2, max_value=max([2, len(st.session_state.chat_session.messages)]),
                               value=2, step=1)
    if st.button(label="Summarize"):
        # update top-level summary and entity list
        st.session_state.chat_session.update_full_summary(0, num_summ)
        # make summary of the message chunk
        st.session_state.chat_session.messages_to_summary(0, num_summ)
        # archive the summarized messages
        st.session_state.chat_session.archive_messages(0, num_summ)

# ====================== Memory Tab ========================

with tab_mem:
    st.session_state.chat_session.prompt_full_summary = st.text_area("Full summary prompt:", 
                                                                     st.session_state.chat_session.prompt_full_summary,
                                                                     height = 100)
    if st.session_state.chat_session.full_summary is None:
        st.write("Full summary: None")
    else:
        st.session_state.chat_session.full_summary = st.text_area("Full summary:", st.session_state.chat_session.full_summary, height = 150)
    
    st.session_state.chat_session.prompt_msg_summary = st.text_area("Message summary prompt:", 
                                                                     st.session_state.chat_session.prompt_msg_summary,
                                                                     height = 100)
    if len(st.session_state.chat_session.message_summaries) == 0:
        st.write("Message chunk summary: None")
    else:
        st.session_state.chat_session.message_summaries[-1]['content'] = st.text_area("Message chunk summary:", 
                                                                           st.session_state.chat_session.message_summaries[-1]['content'],
                                                                           height = 150)
        # embed the updaated message summary
        st.session_state.chat_session.embed_text(st.session_state.chat_session.message_summaries[-1], "message_summary")
    
    st.session_state.chat_session.prompt_entity_list = st.text_area("Entity list prompt:", 
                                                                     st.session_state.chat_session.prompt_entity_list,
                                                                     height = 100)
    if st.session_state.chat_session.entity_list is None:
        st.write("Entity list: None")
    else:
        st.session_state.chat_session.entity_list = st.text_area("Entity list:", st.session_state.chat_session.entity_list, height = 150)
    # display current system prompt
    st.write("System prompt:")
    st.write(st.session_state.chat_session.compile_system_prompt())

# ====================== Database Tab ========================

with tab_db:
    query = st.text_input("Vector database query:")
    if query is not None and query != "":
        db_res = st.session_state.chat_session.query_vector_db(query_text=query, n_results=3)
        st.write(db_res)
    # button to update full vector database
    if st.button(label="Rebuild vector DB"):
        st.session_state.chat_session.update_full_vector_db()
